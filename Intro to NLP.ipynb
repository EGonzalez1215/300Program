{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Language Processing Introduction**\n",
    "**Step through this notebook 1 section at a time.  Read through the text and execute each cell of text or code by using the Run button above.**\n",
    "\n",
    "While watching Marval's Ironman, Tony Stark, wearing his special suit, says, \"Check the brain?\" and the machine replies, \"No sign of cardiac anomaly or unusual brain activity.\" So I ask myself how is that possible? Is that even possible?\n",
    "\n",
    "Maybe a similar question came to you when your grandma asked, \"Hey Alexa, what's the weather today?\" Have you ever seen a customer  chat bot reply to your queries as fast as humans or get curious how your autocomplete works in email? \n",
    "\n",
    "All of this became possible due to recent advancements in Natural Language Processing (NLP). This helpful technique makes a lot of things possible, such as converting any one language to another language. In  parts of the world, India for example, almost every state has its own language and syntax; this kind of machine translation can be quite useful to communicate to a vairety of regions. \n",
    "\n",
    "Lately, the largest value for Natural Language processing has been found in research and development and mergers and aquisitions contract document analysis.  There are so many untested opportunties to apply NLP in our businesses. Here you get a chance to learn about Jupyter notebooks, python code, and natural language processing.\n",
    "\n",
    "Let's get started with the fundamentals of NLP so you can see if you'd like to learn more. \n",
    "\n",
    "One of the applications of NLP is text analysis. For example, we need to find gender bias, sentimental alalysis, or find a particular document from large corpus (a bunch of documents in a collection). Common operations required to do these kind of tasks is to divide a text corpus into words, and then calculate the frequency of those words. \n",
    "\n",
    "Dividing a text corpus into individual words is known as tokenization and those words are called tokens. We will be using the **ntlk library(already installed)** to understand such basic processes for learning purposes. For production purposes, an even faster library such as spaCy can also be used. I will be using various sentences or paragraphs from the popular classic book \"Alice in Wonderland\" as a text corpus to explain common operations for text processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter-\n",
      "[nltk_data]     evergreenalex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentence = \"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do.\"\n",
    "\n",
    "sentence_tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "print (sentence_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens can be received from a paragraph as well. Sometimes there is also a need to break down a paragraph into sentences. That can be done with sent_tokenize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', '“', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', ',', '”', 'thought', 'Alice', '“', 'without', 'pictures', 'or', 'conversations', '?', '”']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "            Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, \n",
    "            but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?”\n",
    "\"\"\"\n",
    "\n",
    "para_tokenized = nltk.word_tokenize(paragraph)\n",
    "\n",
    "print (para_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n            As she said this she looked down at her hands, and was surprised to \\n            see that she had put on one of the Rabbit’s little white kid gloves while she was talking.', '“How can I have done that?” she thought.', '“I must be growing small again.”']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "            As she said this she looked down at her hands, and was surprised to \n",
    "            see that she had put on one of the Rabbit’s little white kid gloves while she was talking. “How can I have done that?” she thought. “I must be growing small again.”\n",
    "\"\"\"\n",
    "\n",
    "para_tokenized = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "print (para_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Normalization**\n",
    "\n",
    "In this section we will learn techniques to get rid of unnecessary words from a corpus. There are different words such as \"was\" or \"can\" that don't hold much value when we don't care much about sentence structure. There is a list of common words such as \"and\", \"is\", \"or\", \"and\", \"a\", \"an\", \"the\", etc., which don't provide much information in a sentence. Usually for text processing we get rid of the words in this list. Here the nltk library comes in handy to do such cleaning of text. There are 21 lists of such common words supported in the nltk library. Since our paragraph is in English, we will use the respective list of stop words from nltk. Also \"As\" and \"as\" are usually considered the same word while counting frequency of words. It is good practice to either make it capital or small letters so that we can get rid of more common words that are just separated by case. Text normalization is an umbrella term for considering the following operations:\n",
    "\n",
    "•Removal of stop words\n",
    "\n",
    "•Stemming - abruptly removing the suffix from words\n",
    "\n",
    "•Converting text to upper or lower case\n",
    "\n",
    "•Lemmatization - replacing a token with its root word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print tokens \n",
      "['As', 'said', 'looked', 'hands', ',', 'surprised', 'see', 'put', 'one', 'Rabbit', '’', 'little', 'white', 'kid', 'gloves', 'talking', '.', '“', 'How', 'I', 'done', '?', '”', 'thought', '.', '“', 'I', 'must', 'growing', 'small', '.', '”']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jupyter-\n",
      "[nltk_data]     evergreenalex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#Download stop words from nltk library\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = nltk.word_tokenize(paragraph)\n",
    "\n",
    "clean_paragraph = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "try:\n",
    "  print (\"print tokens \")\n",
    "  print (clean_paragraph)\n",
    "except:\n",
    "  print (\"expected variable called clean_sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow this is cool ! wow this is awesome.\n",
      "WOW THIS IS COOL ! WOW THIS IS AWESOME.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Wow this is cool ! wow this is awesome.\"\n",
    "lit_sentence = sentence.lower()\n",
    "upp_sentence = sentence.upper()\n",
    "\n",
    "print (lit_sentence)\n",
    "\n",
    "print (upp_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "There are various forms of words in grammer of any language such as differ (verb), different (adjective), differently (adverb), differentness (noun). Both stemming and lemmatization have the same goal of reducing inflectional forms. For example, \"different\", \"differently\", and \"differentness\" will be reduced to \"differ\". \"am\", \"is\", \"are\" will be reduced to \"be\". Stemming usually chops off prefix/suffix from words. For example: \n",
    "\n",
    "Alice's ring has different colors. => Alice ring has differ color.\n",
    "\n",
    "Lemmatization uses morphological analysis to smoothly transform various form of words to a root form, which is called a Lemma. \n",
    "\n",
    "For the word sing, if you use stemming it may return \"s\" after chopping \"ing\", whereas lemmatization returns \"sing\" if the token was as verb. Porter's algorithm is one of the most common and effective algorithms for stemming. Let's see how we can leverage Porter's algorithm for stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Tokens \n",
      "['There', 'might', 'be', 'some', 'sense', 'in', 'your', 'knocking', '.']\n",
      "\n",
      "\n",
      "List of stemmed words\n",
      "['there', 'might', 'be', 'some', 'sens', 'in', 'your', 'knock', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"There might be some sense in your knocking.\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "word_stemmed = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "print (\"List of Tokens \")\n",
    "print (tokens)\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"List of stemmed words\")\n",
    "print (word_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter-\n",
      "[nltk_data]     evergreenalex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'might', 'be', 'some', 'sense', 'in', 'your', 'knocking', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "Lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"There might be some sense in your knocking.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "lemma = [Lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print (lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words\n",
    "![](bag_of_words.png)\n",
    "We will now go through one of the basic techniques in NLP, which is quite useful to search (information retrieval and classification) similar documents from an available corpus of documents. This model is known as the bag-of-words model. It neither cares about grammar nor sequence order. Only the frequency of every word matters. Usually this will be applied after the removal of stop words because we don't want to count the frequency of words that don't provide much information and are there just for the grammar of the language. The frequency of words is used as a feature for the machine learning model. This technique is useful for elementary sentimental analysis, or for basic spam filtering because, usually, there are repetitive positive and negative words to figure out positive/negative sentiment, whereas a lot of spam uses the same words such as \"lottery\", \"win\", etc. We will use `sklearn` to divide sentences into tokens and count frequencies. The `CountVectorizer` function helps us achieve this. Once we use the function, `corpus_vectorizer.vocabulary_` is used to check the number of words (in a technical words vocabulary of corpus) in a sentence. Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Emma was a Catholic because her mother was a Catholic, and Nory’s mother was a Catholic because her father was a Catholic, and her father was a Catholic because his mother was a Catholic, or had been.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus_vectorizer = CountVectorizer()\n",
    "corpus_vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emma': 4,\n",
       " 'was': 12,\n",
       " 'catholic': 3,\n",
       " 'because': 1,\n",
       " 'her': 7,\n",
       " 'mother': 9,\n",
       " 'and': 0,\n",
       " 'nory': 10,\n",
       " 'father': 5,\n",
       " 'his': 8,\n",
       " 'or': 11,\n",
       " 'had': 6,\n",
       " 'been': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show count for each word present in corpus\n",
    "corpus_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `ntlk` library to get split words and get frequency. Let's understand it without using any NLP library. First, the sentence is broken down with white space and then a dictionary is created to keep track of tokens/words. `nltk` performs some more beautification such as removal of extra characters (,;.! etc). For example, see \"nory\" (above) and \"Nory's\" in the following code. `nltk` also converts all letters to lowercase so that \"WORD\" and \"word\" are considered the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Emma was a Catholic because her mother was a Catholic, and Nory’s mother was a Catholic because her father was a Catholic, and her father was a Catholic because his mother was a Catholic, or had been.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Emma': 1, 'was': 6, 'a': 6, 'Catholic': 3, 'because': 3, 'her': 3, 'mother': 3, 'Catholic,': 3, 'and': 2, 'Nory’s': 1, 'father': 2, 'his': 1, 'or': 1, 'had': 1, 'been.': 1}\n"
     ]
    }
   ],
   "source": [
    "#Python code without any nlp library\n",
    "tokens = corpus.split()\n",
    "#Uncomment to see all the tokens\n",
    "#print (tokens)\n",
    "#define empty dictionary \n",
    "dic = {}\n",
    "\n",
    "#save frequency of each token in dictiory\n",
    "for token in tokens:\n",
    "  count = 1\n",
    "  if token not in dic:\n",
    "    dic[token] = count\n",
    "  else:\n",
    "    dic[token] = dic[token] + 1\n",
    "\n",
    "print (dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term-frequency Inverse-document-frequency\n",
    "\n",
    "This section will help you with another feature extraction technique, which emphasizes unique words. TF-IDF stands for term-frequency inverse-document-frequency. It calculates two things: \n",
    "1. **Term Frequency** - (number of times a word _w_ appreared in the document) / (the number of words that appreared in the document)\n",
    "2. **Inverse Document Frequency** - How important the particular word is to the document. It can be derived by the following equation.\n",
    "\n",
    "  IDF(w) = log (Total number of documents / number of documents with word _w_ in it)\n",
    "\n",
    "For example, consider a corpus having 1 million documents and each document is made of 100 words. If the word 'love' appears 10 times in a document and across all the documents, the frequency of the word \"love\" is 10,000:\n",
    "\n",
    "Term Frequency = 10 / 100 = 0.1\n",
    "\n",
    "IDF = log(1000000/10000) = 2\n",
    "\n",
    "TF-IDF = 0.1 * 2 = 0.2\n",
    "\n",
    "Let's understand this concept by emulating a small corpus of documents (though the size of any corpus can be enormous in the real world). We will use `TfidVectorizer` from `sklearn` to convert a raw document in TF-IDF matrix form. To visualize the output better we have created a pandas data frame. \n",
    "\n",
    "Each row represents various words available in the corpus. Each column represents document names. \"cute\" is present the same number of times in all documents across the corpus, so the score is 1 for all and it's similar for other same words. \"she\" is unique across the corpus so it has a higher score for `doc_1`. \"very\" is not common across all documents. For `doc_2`, \"very\" is less important in comparison to `doc_3` where it occured twice. \"she\", \"he\", and \"very\" is important for `doc_1`, `doc_2`, `doc_3`, respectively. \n",
    "\n",
    "In short, the higher the number, the more important term it is for that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF table of corpus summary\n",
      "======================\n",
      "\n",
      "             doc_1     doc_2     doc_3\n",
      "angelina  0.000000  0.000000  1.693147\n",
      "cute      1.000000  1.000000  1.000000\n",
      "he        0.000000  1.693147  0.000000\n",
      "is        1.000000  1.000000  1.000000\n",
      "she       1.693147  0.000000  0.000000\n",
      "very      0.000000  1.287682  2.575364\n"
     ]
    }
   ],
   "source": [
    "doc_1 = \"She is cute\"\n",
    "doc_2 = \"He is very cute\"\n",
    "doc_3 = \"Angelina is very very cute\"\n",
    "\n",
    "corpus = [doc_1, doc_2, doc_3]\n",
    "\n",
    "corpus_preprocess = []\n",
    "\n",
    "for doc in corpus:\n",
    "  corpus_preprocess.append(doc.lower())\n",
    "\n",
    "corpus_vectorizer = TfidfVectorizer(norm=None)\n",
    "tf_idf_scores = corpus_vectorizer.fit_transform(corpus_preprocess)\n",
    "\n",
    "feature_names = corpus_vectorizer.get_feature_names()\n",
    "corpus_index = [doc for doc in corpus_preprocess]\n",
    "\n",
    "print (\"TF-IDF table of corpus summary\")\n",
    "print (\"======================\\n\")\n",
    "\n",
    "print(pd.DataFrame(tf_idf_scores.T.todense(), index = feature_names, columns = [\"doc_1\", \"doc_2\", \"doc_3\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, `CountVectorizer` and `TfidfTransformer` are used to calculate term frequency and inverse document frequency in `TfidfVectorizer`. For a better understanding, a breakdown is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Matrix\n",
      "=====================\n",
      "[[0 1 0 1 1 0]\n",
      " [0 1 1 1 0 1]\n",
      " [1 1 0 1 0 2]]\n",
      "=====================\n",
      "\n",
      "Inverse Document Frequency(common or rare) table\n",
      "=========\n",
      "               IDF\n",
      "angelina  1.693147\n",
      "cute      1.000000\n",
      "he        1.693147\n",
      "is        1.000000\n",
      "she       1.693147\n",
      "very      1.287682\n",
      "\n",
      "Term Frequency-IDF matrix\n",
      "=============\n",
      "\n",
      "[[0.         1.         0.         1.         1.69314718 0.        ]\n",
      " [0.         1.         1.69314718 1.         0.         1.28768207]\n",
      " [1.69314718 1.         0.         1.         0.         2.57536414]]\n"
     ]
    }
   ],
   "source": [
    "#Part 1 - Calculating Term Frequencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "term_frequency = vectorizer.fit_transform(corpus_preprocess)\n",
    "\n",
    "#uncomment following to check feature names\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "print (\"Term Frequency Matrix\")\n",
    "print (\"=====================\")\n",
    "print (term_frequency.toarray())\n",
    "print (\"=====================\")\n",
    "#Part 2 - Calculating Inverse Document Frequency\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = TfidfTransformer(norm=None)\n",
    "vectorizer.fit(term_frequency)\n",
    "\n",
    "idf = vectorizer.idf_\n",
    "\n",
    "df = pd.DataFrame(idf, index = feature_names, columns=['IDF'])\n",
    "print (\"\\nInverse Document Frequency(common or rare) table\")\n",
    "print (\"=========\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "tf_idf_scores = vectorizer.fit_transform(term_frequency.toarray())\n",
    "\n",
    "\n",
    "print (\"\\nTerm Frequency-IDF matrix\")\n",
    "print (\"=============\\n\")\n",
    "print (tf_idf_scores.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now hopefully you can see how easy it is to follow along with someone elses analysis using a Juypyter notebook and python code libraries. There are plenty more levers to pull in NLP solutions and a great area to learn more about how to customize for your needs and automate with OCR.\n",
    "\n",
    "Thanks for following along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
